# 优化记录

## V0.3

### 问题1
线程创建过多，创建线程函数的等待时间过长

发现将多层循环全部并行化对线程创建函数等待时间有损害
但适当的多层化有一定帮助

所以从目前看来，线程的创建有点多了

### 问题2
内存访问优化

### 尝试方向
    1.重构整个函数，全部放在一个并行区内，但这样会减少代码的模块化

    2.提高内存访问效率



### v0.3.0

将simd指令交给openMP降低了内存的读取次数，约10%



### v0.3.1
注意到z0~z6的多次读取，因此将其存放于结构体内，提高存储的连续性
发现内存读取次数下降了32%，但缓存命中率翻倍了，最终运行时间相近

在内存带宽利用率方面，低带宽区域变大，平局带宽下降25%，高带宽峰值显著下降
因此这一方面有所改进

追求高CPU利用率只有在程序是计算受限而非内存受限时才是最有效的策略。最终目标是更高的性能，而非更高的CPU利用率。



#### v0.3.1.1
折中方案，将z6变为独立变量，这时LLC Miss变为0，但读取次数变多
带宽利用率分布接近,放弃

*** 问题 ***
：目前优化内存并没有对时间有显著提升，要思考以下问题
很可能的原因是，并行等待时间仍然占据大部分时间



### v0.3.1.2
尝试将filtertransform求和顺序交换，提高顺序读取速率，但效果不明显
不少性能参数甚至变差了



### v0.3.2
思路来自 https://zhuanlan.zhihu.com/p/438173915
重大突破：优化sgemm算法，全部分块，向量化，当前速度可达0.055197
但应该可以进一步优化，方法待定

发现：并行度下降，可以考虑如何优化

当前加速比：8.86x



### v0.3.2
转移至集群，单节点64核
当前加速比：60.0x

线程是使用情况依然不理想，但有一定进步，存在长期16线程工作情况

内存使用状况与第一次接近，事实上没有什么优化

hotspot热点重点在sgemm，其次是output——unpacking

***尝试1*** 修改NUMA节点数 -v0.3.2.8

        结论：尽量采用多节点

***尝试2*** 降低主函数并行次数，提高sgemm并行次数

        发现：sgemm性能折损严重，因为sgemm当前效率并不高，主程序并行减少损耗更大
        尝试做一些均衡
            手动优化：主程序32，sgemm 4 到达最高性能的83%

        得到了一个令人遗憾的结果：sgemm的omp并行部分几乎是负优化(12-17)
        下面进行vtune分析
        发现曲线和附带openmp优化的sgemm曲线一致

        所以说sgemm函数亟待优化
            可行方案1：CUDA加速
                    2：MPI加速

### v0.3.3
    考虑到更大的核心数，将outputimagestore的循环线程取满，速度提升了25%
    但这是一个饮鸩止渴的方法，本质上仍然加重了cpu负荷

    当前速度417.43GFLops 加速比：80.00x

    下面是vtune报告分析
        可以看到：平均cpu利用率下降了，这正是强行collapse的结果
        平均线程数也从9变成了8

### v0.3.3.4
    局部优化：将outputtrans两个循环分开，提高并行度
    调换后，并行度提升10%
    几个输入输出函数运行时间下降40%

    当前最佳算力 445GFlops  加速比：84.04x，提升6%


## v0.4
    尝试使用CUDA对矩阵加速，这时采用的是单个矩阵乘法加速
    但很遗憾的是，这一函数的调用和信息传递的成本实在太大了
    关键在于：调用CUDA库的时间至少要0.3s，这样对于小测试集是得不偿失的

### v0.4.5
    现在只能勉强保证正确性，但实际性能以及具体细节实现差太多了
    1.矩阵应当分批次处理，不要一次传入过大的矩阵

    2.去掉循环体，减少函数调用

    3.具体来说不应当是过大，而是不能内存对齐的矩阵，因为gpu会自行选择计算核，如果矩阵形状不合适，将会报错
    所以考虑提前将矩阵规范化处理，分块成适当大小的矩阵

    应当进一步了解cublas原理，不然现在的效率是很低的

### v0.4.9
    成功保证正确性，然而遗憾的是性能损耗很大，CUDA加速比约为42%
    下面通过Vtune观察调度的性能消耗
    其他函数性能没变
    但是gpu矩阵乘法很慢

尝试1：将内存装载并行化，但效率下降

尝试2：将小矩阵交给cpu运算,但这时不能全局并行，损失太大

nsys数据说明，最大的性能瓶颈在数据传输上
所以我们期望减少内存分配与销毁的次数，用尽可能少的次数传输大量数据，然后再大批量的传输回来

## v0.5.0
    尝试使用batched处理，目前看来读写次数仍然过多
    小优化：只做一次初始化，当前速度45.88x

尝试方向1：
    采用pinned memory

    参考；https://blog.csdn.net/guaaaaaaa/article/details/130876447
        https://developer.nvidia.com/zh-cn/blog/how-optimize-data-transfers-cuda-cc/

    方向1.1：采用页锁定和异步流方法结合
        有一定优化效果(v0.5.1)-(v0.5.0-batched) - (v0.4.9)
    https://zhuanlan.zhihu.com/p/667225351
    https://zhuanlan.zhihu.com/p/590077188
    

尝试方向2：
    半合并，即局部分块处理，避免数据过大超出带宽

尝试方向3：
    彻底在gpu上运行，消除大量通信成本

## v0.5.1
    采用5.0尝试1，此外考虑到内存的多次分配问题，直接创建一整个内存池
    避免多次重复分配
    很好的解决了内存读取问题，

    同时考虑到矩阵计算

    当前加速比：115x 算力：612.77GFlops
    一个更好的加速比：127.88x 算力：678GFlops

    不过，小测试集cpu跑的还是比gpu快的多，所以仍可以考虑分类

## v0.5.2
     添加小批量卷积的单独情形，这时采用cpu计算
     不过，不要期望大批量计算中第一层比cpu结果慢的原因不是矩阵调小了，而是创建内存池消耗时间

### v0.5.2.9
    期望预先创建足够大的内存池，但是似乎效率更低了
    因为不是所有层的所有方向都要很大的内存，很难做一般性优化
    不过第二层提升了不少，但第一层下降了50%


## v0.6

### v0.6.0
    cuda流同步事实上花费了大量时间，但又不能去掉，不然可能发生数据混乱
    但这说明了潜在的优化方案：削减同步时间

### v0.6.2
    cuda分片处理，效率有一定的提升
    采用16stream性能较佳，
    加速比为136.5x  算力 723.12GFlops
    同时采用多句柄做进一步优化，性能提升5%
    同步量略微降低

### v0.6.3
    考虑采用多缓冲机制，当前最大瓶颈仍是传输
    其次则是同步，理论上不同步可以让加速比到160x，但是那样会出错

尝试2：
    可以考虑在矩阵变换之初就把数据传输到gpu中，不过这样就会削减cpu计算的并行的

尝试3：
    乒乓操作，cpu与gpu操作在同一区间交替进行

尝试4：
    采用fp16进度计算，但很可惜的是，大规模计算的时候直接溢出了
    同样采用bf16精度计算，结果也直接溢出了
    不知道为什么cublassgemm函数在使用bf16类型的情况下直接结果为0了



## v0.7

### v0.7.0-4
    尝试写结合内存，但是传输效率几乎没有提升，cpu内通信成本大大增加

### 0.7.5
    尝试将前面的函数512位向量化，性能提升5%
    当前最高加速比130x 691.62GFlops