# 优化记录

## V0.3

### 问题1
线程创建过多，创建线程函数的等待时间过长

发现将多层循环全部并行化对线程创建函数等待时间有损害
但适当的多层化有一定帮助

所以从目前看来，线程的创建有点多了

### 问题2
内存访问优化

### 尝试方向
    1.重构整个函数，全部放在一个并行区内，但这样会减少代码的模块化

    2.提高内存访问效率



### v0.3.0

将simd指令交给openMP降低了内存的读取次数，约10%



### v0.3.1
注意到z0~z6的多次读取，因此将其存放于结构体内，提高存储的连续性
发现内存读取次数下降了32%，但缓存命中率翻倍了，最终运行时间相近

在内存带宽利用率方面，低带宽区域变大，平局带宽下降25%，高带宽峰值显著下降
因此这一方面有所改进

追求高CPU利用率只有在程序是计算受限而非内存受限时才是最有效的策略。最终目标是更高的性能，而非更高的CPU利用率。



#### v0.3.1.1
折中方案，将z6变为独立变量，这时LLC Miss变为0，但读取次数变多
带宽利用率分布接近,放弃

*** 问题 ***
：目前优化内存并没有对时间有显著提升，要思考以下问题
很可能的原因是，并行等待时间仍然占据大部分时间



### v0.3.1.2
尝试将filtertransform求和顺序交换，提高顺序读取速率，但效果不明显
不少性能参数甚至变差了



### v0.3.2
思路来自 https://zhuanlan.zhihu.com/p/438173915
重大突破：优化sgemm算法，全部分块，向量化，当前速度可达0.055197
但应该可以进一步优化，方法待定

发现：并行度下降，可以考虑如何优化

当前加速比：8.86x



### v0.3.2
转移至集群，单节点64核
当前加速比：60.0x

线程是使用情况依然不理想，但有一定进步，存在长期16线程工作情况

内存使用状况与第一次接近，事实上没有什么优化

hotspot热点重点在sgemm，其次是output——unpacking

***尝试1*** 修改NUMA节点数 -v0.3.2.8

        结论：尽量采用多节点

***尝试2*** 降低主函数并行次数，提高sgemm并行次数

        发现：sgemm性能折损严重，因为sgemm当前效率并不高，主程序并行减少损耗更大
        尝试做一些均衡
            手动优化：主程序32，sgemm 4 到达最高性能的83%

        得到了一个令人遗憾的结果：sgemm的omp并行部分几乎是负优化(12-17)
        下面进行vtune分析
        发现曲线和附带openmp优化的sgemm曲线一致

        所以说sgemm函数亟待优化
            可行方案1：CUDA加速
                    2：MPI加速

### v0.3.3
    考虑到更大的核心数，将outputimagestore的循环线程取满，速度提升了25%
    但这是一个饮鸩止渴的方法，本质上仍然加重了cpu负荷

    当前速度417.43GFLops 加速比：80.00x

    下面是vtune报告分析
        可以看到：平均cpu利用率下降了，这正是强行collapse的结果
        平均线程数也从9变成了8

### v0.3.3.4
    局部优化：将outputtrans两个循环分开，提高并行度
    调换后，并行度提升10%
    几个输入输出函数运行时间下降40%

    当前最佳算力 445GFlops  加速比：84.04x，提升6%


## v0.4
    尝试使用CUDA对矩阵加速，这时采用的是单个矩阵乘法加速
    但很遗憾的是，这一函数的调用和信息传递的成本实在太大了
    关键在于：调用CUDA库的时间至少要0.3s，这样对于小测试集是得不偿失的

### v0.4.5
    现在只能勉强保证正确性，但实际性能以及具体细节实现差太多了
    1.矩阵应当分批次处理，不要一次传入过大的矩阵

    2.去掉循环体，减少函数调用

    3.具体来说不应当是过大，而是不能内存对齐的矩阵，因为gpu会自行选择计算核，如果矩阵形状不合适，将会报错
    所以考虑提前将矩阵规范化处理，分块成适当大小的矩阵

    应当进一步了解cublas原理，不然现在的效率是很低的