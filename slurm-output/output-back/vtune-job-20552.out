使用版本: v0.3.2.71
使用大测试集 (VGG16)
错误: VTune 未能成功加载，尝试其他加载方法...
VTune 版本信息：
Intel(R) VTune(TM) Profiler 2024.1.0 (build 627630) Command Line Tool
Copyright (C) 2009 Intel Corporation. All rights reserved.
===============================================
 VTune 分析报告 - 版本: v0.3.2.71 - 测试集: vgg16
 开始时间: Tue Mar 25 04:36:04 PM CST 2025
===============================================



--- 开始 hotspots 分析 Tue Mar 25 04:36:04 PM CST 2025 ---
收集数据中，请等待...
Layer 0 :  Elapse time 1238.782724 ms. (    8.80 GFlops) 
Layer 1 :  Elapse time 2015.118996 ms. (  115.40 GFlops) 
Layer 2 :  Elapse time 911.725283 ms. (  125.25 GFlops) 
Layer 3 :  Elapse time 1219.633341 ms. (  187.25 GFlops) 
Layer 4 :  Elapse time 639.639695 ms. (  172.09 GFlops) 
Layer 5 :  Elapse time 1095.676661 ms. (  200.93 GFlops) 
Layer 6 :  Elapse time 1074.230671 ms. (  204.94 GFlops) 
Layer 7 :  Elapse time 1018.325647 ms. (  216.19 GFlops) 
Layer 8 :  Elapse time 534.052690 ms. (  191.13 GFlops) 
Layer 9 :  Elapse time 825.112661 ms. (  247.41 GFlops) 
Layer 10:  Elapse time 860.243003 ms. (  237.31 GFlops) 
Layer 11:  Elapse time 986.497720 ms. (  206.94 GFlops) 
Layer 12:  Elapse time 282.301029 ms. (  154.04 GFlops) 
Layer 13:  Elapse time 262.530963 ms. (  165.64 GFlops) 
Layer 14:  Elapse time 305.683692 ms. (  142.26 GFlops) 
Layer 15:  Elapse time 240.484635 ms. (  180.83 GFlops) 
Total elapse time: 13.510039. (  166.17 GFlops) 
Elapsed Time: 41.634s
    CPU Time: 369.730s
        Effective Time: 369.730s
        Spin Time: 0s
        Overhead Time: 0s
    Total Thread Count: 3,136
    Paused Time: 0s

Top Hotspots
Function                          Module    CPU Time  % of CPU Time(%)
--------------------------------  --------  --------  ----------------
sgemm._omp_fn.0                   winograd  160.684s             43.5%
output_unpacking_store._omp_fn.0  winograd   77.730s             21.0%
image_transform._omp_fn.0         winograd   47.192s             12.8%
output_transform._omp_fn.0        winograd   46.520s             12.6%
image_packing._omp_fn.0           winograd   22.060s              6.0%
[Others]                          N/A        15.544s              4.2%
Effective CPU Utilization: 15.7%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 10.058 out of 64
Collection and Platform Info
    Application Command Line: numactl "--cpunodebind=0" "--membind=0" "./winograd" "conf/vgg16.conf" 
    Operating System: 5.15.0-100-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
    Computer Name: hepnode1
    Result Size: 15.8 MB 
    Collection start time: 08:36:08 25/03/2025 UTC
    Collection stop time: 08:36:49 25/03/2025 UTC
    Collector Type: Driverless Perf per-process counting,User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Icelake
        Frequency: 1.995 GHz
        Logical CPU Count: 64
        LLC size: 50.3 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
数据收集成功！生成报告...
分析完成: hotspots
结果保存在: vtune_results/v0.3.2.71-vgg16/hotspots_vgg16_20250325_163604



--- 开始 memory-access 分析 Tue Mar 25 04:37:13 PM CST 2025 ---
收集数据中，请等待...
Layer 0 :  Elapse time 1089.035670 ms. (   10.01 GFlops) 
Layer 1 :  Elapse time 1978.859981 ms. (  117.52 GFlops) 
Layer 2 :  Elapse time 750.639041 ms. (  152.12 GFlops) 
Layer 3 :  Elapse time 1088.509003 ms. (  209.81 GFlops) 
Layer 4 :  Elapse time 473.673662 ms. (  232.39 GFlops) 
Layer 5 :  Elapse time 870.004654 ms. (  253.05 GFlops) 
Layer 6 :  Elapse time 870.067994 ms. (  253.03 GFlops) 
Layer 7 :  Elapse time 814.686696 ms. (  270.23 GFlops) 
Layer 8 :  Elapse time 369.320631 ms. (  276.38 GFlops) 
Layer 9 :  Elapse time 643.914302 ms. (  317.04 GFlops) 
Layer 10:  Elapse time 642.592986 ms. (  317.69 GFlops) 
Layer 11:  Elapse time 645.502965 ms. (  316.26 GFlops) 
Layer 12:  Elapse time 131.423950 ms. (  330.89 GFlops) 
Layer 13:  Elapse time 132.956982 ms. (  327.07 GFlops) 
Layer 14:  Elapse time 130.166292 ms. (  334.08 GFlops) 
Layer 15:  Elapse time 129.468997 ms. (  335.88 GFlops) 
Total elapse time: 10.760824. (  208.63 GFlops) 
Elapsed Time: 32.822s
    CPU Time: 261.904s
    Memory Bound: 100.0% of Pipeline Slots
     | The metric value is high. This may indicate that a significant fraction
     | of execution pipeline slots could be stalled due to demand memory load
     | and stores. Explore the metric breakdown by memory hierarchy, memory
     | bandwidth information, and correlation by memory objects.
     |
        L1 Bound: 13.7% of Clockticks
         | This metric shows how often machine was stalled without missing the
         | L1 data cache. The L1 cache typically has the shortest latency.
         | However, in certain cases like loads blocked on older stores, a load
         | might suffer a high latency even though it is being satisfied by the
         | L1.
         |
        L2 Bound: 5.1% of Clockticks
         | This metric shows how often machine was stalled on L2 cache. Avoiding
         | cache misses (L1 misses/L2 hits) will improve the latency and
         | increase performance.
         |
        L3 Bound: 1.9% of Clockticks
        DRAM Bound: 1.5% of Clockticks
            DRAM Bandwidth Bound: 0.5% of Elapsed Time
        Store Bound: 7.8% of Clockticks
        NUMA: % of Remote Accesses: 0.0%
        UPI Utilization Bound: 0.0% of Elapsed Time
    Loads: 202,592,046,146
    Stores: 41,923,042,896
    LLC Miss Count: 108,708,740
        Local Memory Access Count: 122,522,428
        Remote Memory Access Count: 0
        Remote Cache Access Count: 0
    Total Thread Count: 3,138
    Paused Time: 0s

Bandwidth Utilization
Bandwidth Domain                  Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
--------------------------------  ----------------  ----------------  -------  ---------------------------------------------
DRAM, GB/sec                      180                        113.100   10.358                                           0.0%
DRAM Single-Package, GB/sec       90                         113.000   10.392                                           0.5%
UPI Utilization Single-link, (%)  100                          1.400    0.009                                           0.0%
Collection and Platform Info
    Application Command Line: numactl "--cpunodebind=0" "--membind=0" "./winograd" "conf/vgg16.conf" 
    Operating System: 5.15.0-100-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
    Computer Name: hepnode1
    Result Size: 257.6 MB 
    Collection start time: 08:37:31 25/03/2025 UTC
    Collection stop time: 08:38:04 25/03/2025 UTC
    Collector Type: Driverless Perf system-wide sampling
    CPU
        Name: Intel(R) Xeon(R) Processor code named Icelake
        Frequency: 1.995 GHz
        Logical CPU Count: 64
        Max DRAM Single-Package Bandwidth: 90.000 GB/s
        LLC size: 50.3 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
数据收集成功！生成报告...
分析完成: memory-access
结果保存在: vtune_results/v0.3.2.71-vgg16/memory-access_vgg16_20250325_163604



--- 开始 threading 分析 Tue Mar 25 04:38:23 PM CST 2025 ---
收集数据中，请等待...
Layer 0 :  Elapse time 1217.540344 ms. (    8.95 GFlops) 
Layer 1 :  Elapse time 2081.869682 ms. (  111.70 GFlops) 
Layer 2 :  Elapse time 890.053352 ms. (  128.30 GFlops) 
Layer 3 :  Elapse time 1187.167645 ms. (  192.37 GFlops) 
Layer 4 :  Elapse time 590.335687 ms. (  186.46 GFlops) 
Layer 5 :  Elapse time 931.323369 ms. (  236.38 GFlops) 
Layer 6 :  Elapse time 1013.056676 ms. (  217.31 GFlops) 
Layer 7 :  Elapse time 1000.942389 ms. (  219.94 GFlops) 
Layer 8 :  Elapse time 494.178613 ms. (  206.55 GFlops) 
Layer 9 :  Elapse time 782.014688 ms. (  261.05 GFlops) 
Layer 10:  Elapse time 818.468332 ms. (  249.42 GFlops) 
Layer 11:  Elapse time 813.695669 ms. (  250.89 GFlops) 
Layer 12:  Elapse time 277.480682 ms. (  156.72 GFlops) 
Layer 13:  Elapse time 263.708353 ms. (  164.90 GFlops) 
Layer 14:  Elapse time 240.967671 ms. (  180.47 GFlops) 
Layer 15:  Elapse time 196.316004 ms. (  221.51 GFlops) 
Total elapse time: 12.799119. (  175.40 GFlops) 
Elapsed Time: 39.530s
    Paused Time: 0s
Effective CPU Utilization: 14.5% (9.292 out of 64 logical CPUs)
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Total Thread Count: 3,137
        Thread Oversubscription: 0s (0.0% of CPU Time)
    Wait Time with poor CPU Utilization: 6.682s (100.0% of Wait Time)

        Top Waiting Objects
        Sync Object                                                                   Wait Time with poor CPU Utilization  (% from Object Wait Time)(%)  Wait Count
        ----------------------------------------------------------------------------  -----------------------------------  ----------------------------  ----------
        Thread Pool                                                                                                6.682s                        100.0%       2,112
        Stream conf/vgg16.conf 0x4e2c38dc                                                                          0.000s                        100.0%           1
        Stream /proc/self/status 0x3791b3a6                                                                        0.000s                        100.0%           1
        Stream /sys/devices/system/cpu/cpu0/topology/core_siblings_list 0x69712aff                                 0.000s                        100.0%           1
        Stream /sys/devices/system/cpu/cpu7/topology/thread_siblings_list 0x8b73d7cf                               0.000s                        100.0%           1
        [Others]                                                                                                   0.000s                        100.0%          21
    Spin and Overhead Time: 0s (0.0% of CPU Time)

        Top Functions with Spin or Overhead Time
        Function  Module  Spin and Overhead Time  (% from CPU Time)(%)
        --------  ------  ----------------------  --------------------
Collection and Platform Info
    Application Command Line: numactl "--cpunodebind=0" "--membind=0" "./winograd" "conf/vgg16.conf" 
    Operating System: 5.15.0-100-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
    Computer Name: hepnode1
    Result Size: 17.9 MB 
    Collection start time: 08:38:25 25/03/2025 UTC
    Collection stop time: 08:39:05 25/03/2025 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Icelake
        Frequency
        Logical CPU Count: 64
        LLC size: 50.3 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
数据收集成功！生成报告...
分析完成: threading
结果保存在: vtune_results/v0.3.2.71-vgg16/threading_vgg16_20250325_163604



--- 开始 uarch-exploration 分析 Tue Mar 25 04:39:28 PM CST 2025 ---
收集数据中，请等待...
Layer 0 :  Elapse time 1098.771016 ms. (    9.92 GFlops) 
Layer 1 :  Elapse time 1927.016338 ms. (  120.68 GFlops) 
Layer 2 :  Elapse time 753.709316 ms. (  151.50 GFlops) 
Layer 3 :  Elapse time 1086.333036 ms. (  210.23 GFlops) 
Layer 4 :  Elapse time 433.457692 ms. (  253.95 GFlops) 
Layer 5 :  Elapse time 864.204645 ms. (  254.74 GFlops) 
Layer 6 :  Elapse time 809.284687 ms. (  272.03 GFlops) 
Layer 7 :  Elapse time 811.643283 ms. (  271.24 GFlops) 
Layer 8 :  Elapse time 367.023627 ms. (  278.11 GFlops) 
Layer 9 :  Elapse time 645.912011 ms. (  316.06 GFlops) 
Layer 10:  Elapse time 641.138951 ms. (  318.41 GFlops) 
Layer 11:  Elapse time 643.969297 ms. (  317.01 GFlops) 
Layer 12:  Elapse time 149.950345 ms. (  290.01 GFlops) 
Layer 13:  Elapse time 131.369273 ms. (  331.03 GFlops) 
Layer 14:  Elapse time 140.243689 ms. (  310.08 GFlops) 
Layer 15:  Elapse time 132.638613 ms. (  327.86 GFlops) 
Total elapse time: 10.636666. (  211.06 GFlops) 
Elapsed Time: 32.446s
    Clockticks: 962,370,000,000
    Instructions Retired: 1,456,380,000,000
    CPI Rate: 0.661
    Retiring: 100.0% of Pipeline Slots
     | A high fraction of pipeline slots was utilized by useful work. While the
     | goal is to make this metric value as big as possible, a high Retiring
     | value for non-vectorized code could prompt you to consider code
     | vectorization. Vectorization enables doing more computations without
     | significantly increasing the number of instructions, thus improving the
     | performance. Note that this metric value may be highlighted due to
     | Microcode Sequencer (MS) issue, so the performance can be improved by
     | avoiding using the MS.
     |
        Light Operations: 100.0% of Pipeline Slots
         | CPU retired light-weight operations (ones which require no more than
         | one uop) in a significant fraction of cycles. This correlates with
         | total number of instructions used by the program. Optimum value of
         | uops-per-instruction ratio is 1. While this is the most desirable
         | metric, high values can also provide opportunities for performance
         | optimizations.
         |
            FP Arithmetic: 0.0% of uOps
                FP x87: 0.0% of uOps
                FP Scalar: 0.0% of uOps
                FP Vector: 0.0% of uOps
                    128-bit FP Vector: 0.6% of uOps
                    256-bit FP Vector: 17.2% of uOps
                    512-bit FP Vector: 0.0% of uOps
            Memory Operations: 100.0% of Pipeline Slots
             | For a significant fraction of pipeline slots the CPU was retiring
             | memory operations - uops for memory load or store accesses.
             |
            Branch Instructions: 100.0% of Pipeline Slots
             | For a significant fraction of pipeline slots the CPU was retiring
             | branch instructions.
             |
            Nop Instructions: 23.8% of Pipeline Slots
             | For a significant fraction of pipeline slots the CPU was retiring
             | NOP (no op) instructions. Compilers often use NOPs for certain
             | address alignments - e.g. start address of a function or loop
             | body.
             |
            Other: 100.0% of Pipeline Slots
             | This metric represents a non-floating-point (FP) uop fraction the
             | CPU has executed. If your application has no FP operations, this
             | is likely to be the biggest fraction.
             |
        Heavy Operations: 100.0% of Pipeline Slots
         | CPU retired heavy-weight operations (instructions that required 2+
         | uops) in a significant fraction of cycles.
         |
            Few Uops Instructions: 100.0% of Pipeline Slots
             | This metric represents fraction of slots where the CPU was
             | retiring instructions that that are decoder into two or up to
             | ([SNB+] four; [ADL+] five) uops. This highly-correlates with the
             | number of uops in such instructions.
             |
            Microcode Sequencer: 36.8% of Pipeline Slots
             | Issue: A significant fraction of cycles was spent retiring uOps
             | fetched by the Microcode Sequencer.
             | 
             | Tips:
             | 
             | 1. Make sure the /arch compiler flags are correct.
             | 
             | 2. Check the child Assists metric and, if it is highlighted as an
             | issue, follow the provided recommendations.
             | 
             | Note that this metric value may be highlighted due to MS Switches
             | issue.
             |
                Assists: 1.0% of Pipeline Slots
                CISC: 35.8% of Pipeline Slots
                 | For a significant fraction of cycles the CPU retired uops
                 | originated from CISC (complex instruction set computer)
                 | instruction. A CISC instruction has multiple uops that are
                 | required to perform the instruction's functionality as in the
                 | case of read-modify-write as an example. Since these
                 | instructions require mutiple uops they may or may not imply
                 | sub-optimal use of machine resources.
                 |
    Front-End Bound: 100.0% of Pipeline Slots
     | Issue: A significant portion of Pipeline Slots is remaining empty due to
     | issues in the Front-End.
     | 
     | Tips:  Make sure the code working size is not too large, the code layout
     | does not require too many memory accesses per cycle to get enough
     | instructions for filling four pipeline slots, or check for microcode
     | assists.
     |
        Front-End Latency: 100.0% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end latency issues, such as instruction-cache
         | misses, ITLB misses or fetch stalls after a branch misprediction. In
         | such cases, the front-end delivers no uOps.
         |
            ICache Misses: 0.0% of Clockticks
            ITLB Overhead: 0.1% of Clockticks
            Branch Resteers: 0.5% of Clockticks
                Mispredicts Resteers: 0.5% of Clockticks
                Clears Resteers: 0.0% of Clockticks
                Unknown Branches: 0.0% of Clockticks
            DSB Switches: 0.2% of Clockticks
             | Issue: A significant portion of cycles is spent switching from
             | the DSB to the MITE.  This may happen if a hot code region is too
             | large to fit into the DSB.
             | 
             | Tips: Consider changing code layout (for example, via profile-
             | guided optimization) to help your hot regions fit into the DSB.
             | 
             | See the "Optimization for Decoded ICache" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            Length Changing Prefixes: 0.0% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Length
             | Changing Prefixes (LCPs).
             | 
             | Tips: To avoid this issue, use proper compiler flags. Intel
             | Compiler enables these flags by default.
             | 
             | See the "Length-Changing Prefixes (LCP)" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            MS Switches: 3.8% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to
             | switches of uOp delivery to the Microcode Sequencer (MS).
             | Commonly used instructions are optimized for delivery by the DSB
             | or MITE pipelines. Certain operations cannot be handled natively
             | by the execution pipeline, and must be performed by microcode
             | (small programs injected into the execution stream). Switching to
             | the MS too often can negatively impact performance. The MS is
             | designated to deliver long uOp flows required by CISC
             | instructions like CPUID, or uncommon conditions like Floating
             | Point Assists when dealing with Denormals. Note that this metric
             | value may be highlighted due to Microcode Sequencer issue.
             |
        Front-End Bandwidth: 72.2% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end bandwidth issues, such as inefficiencies in
         | the instruction decoders or code restrictions for caching in the DSB
         | (decoded uOps cache). In such cases, the front-end typically delivers
         | a non-optimal amount of uOps to the back-end.
         |
            Front-End Bandwidth MITE: 1.8% of Pipeline Slots
                Decoder-0 Alone: 0.5% of Pipeline Slots
                %MITE_4wide: 0.3% of Clockticks
            Front-End Bandwidth DSB: 13.5% of Pipeline Slots
            (Info) DSB Coverage: 62.7%
             | Issue: A significant fraction of uOps was not delivered by the
             | DSB (known as Decoded ICache or uOp Cache). This may happen if a
             | hot code region is too large to fit into the DSB.
             | 
             | Tips: Consider changing the code layout (for example, via
             | profile-guided optimization) to help your hot regions fit into
             | the DSB.
             | 
             | See the "Optimization for Decoded ICache" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            (Info) DSB Misses: 100.0% of Pipeline Slots
             | %DSB_MissesIssueTextAll
             |
    Bad Speculation: 0.0% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 0.0% of Pipeline Slots
    Back-End Bound: 100.0% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 100.0% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 12.4% of Clockticks
             | This metric shows how often machine was stalled without missing
             | the L1 data cache. The L1 cache typically has the shortest
             | latency. However, in certain cases like loads blocked on older
             | stores, a load might suffer a high latency even though it is
             | being satisfied by the L1. Note that this metric value may be
             | highlighted due to DTLB Overhead or Cycles of 1 Port Utilized
             | issues.
             |
                DTLB Overhead: 100.0% of Clockticks
                 | Issue: A significant portion of cycles is being spent
                 | handling first-level data TLB misses.
                 | 
                 | Tips:
                 | 
                 | 1.  As with ordinary data caching, focus on improving data
                 | locality and reducing the working-set size to minimize the
                 | DTLB overhead.
                 | 
                 | 2. Consider using profile-guided optimization (PGO) to
                 | collocate frequently-used data on the same page.
                 | 
                 | 3. Try using larger page sizes for large amounts of
                 | frequently-used data.
                 |
                    Load STLB Hit: 100.0% of Clockticks
                     | In significant fraction of cycles the (first level) DTLB
                     | was missed by load accesses, that later on hit in second-
                     | level TLB (STLB).
                     |
                    Load STLB Miss: 0.5% of Clockticks
                Loads Blocked by Store Forwarding: 0.2% of Clockticks
                Lock Latency: 0.1% of Clockticks
                 | A significant fraction of CPU cycles spent handling cache
                 | misses due to lock operations. Due to the microarchitecture
                 | handling of locks, they are classified as L1 Bound regardless
                 | of what memory source satisfied them. Note that this metric
                 | value may be highlighted due to Store Latency issue.
                 |
                Split Loads: 6.3% of Clockticks
                4K Aliasing: 20.9% of Clockticks
                 | Issue: A significant proportion of cycles is spent dealing
                 | with false 4k aliasing between loads and stores.
                 | 
                 | Tips: Use the source/assembly view to identify the aliasing
                 | loads and stores, and then adjust your data layout so that
                 | the loads and stores no longer alias.
                 | 
                 |  See the Intel 64 and IA-32 Architectures Optimization
                 | Reference Manual for more details.
                 |
                FB Full: 1.7% of Clockticks
                 | This metric does a rough estimation of how often L1D Fill
                 | Buffer unavailability limited additional L1D miss memory
                 | access requests to proceed. The higher the metric value, the
                 | deeper the memory hierarchy level the misses are satisfied
                 | from. Often it hints on approaching bandwidth limits (to L2
                 | cache, L3 cache or external memory). Avoid adding software
                 | prefetches if indeed memory BW limited.
                 |
            L2 Bound: 6.7% of Clockticks
             | This metric shows how often machine was stalled on L2 cache.
             | Avoiding cache misses (L1 misses/L2 hits) will improve the
             | latency and increase performance.
             |
            L3 Bound: 2.7% of Clockticks
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 2.3% of Clockticks
                L3 Latency: 2.0% of Clockticks
                SQ Full: 2.9% of Clockticks
            DRAM Bound: 1.9% of Clockticks
                Memory Bandwidth: 3.5% of Clockticks
                Memory Latency: 10.5% of Clockticks
                    Local Memory: 4.7% of Clockticks
                    Remote Memory: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 9.2% of Clockticks
                Store Latency: 28.9% of Clockticks
                False Sharing: 9.0% of Clockticks
                Split Stores: 0.1%
                Streaming Stores: 0.0% of Clockticks
                DTLB Store Overhead: 28.4% of Clockticks
                    Store STLB Hit: 26.9% of Clockticks
                    Store STLB Miss: 1.5% of Clockticks
        Core Bound: 100.0% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 14.3% of Clockticks
            Port Utilization: 100.0% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 2.4% of Clockticks
                    Serializing Operations: 18.0% of Clockticks
                        Slow Pause: 8.3% of Clockticks
                    Mixing Vectors: 0.0% of Clockticks
                Cycles of 1 Port Utilized: 18.2% of Clockticks
                 | This metric represents cycles fraction where the CPU executed
                 | total of 1 uop per cycle on all execution ports (Logical
                 | Processor cycles since ICL, Physical Core cycles otherwise).
                 | This can be due to heavy data-dependency among software
                 | instructions, or oversubscribing a particular hardware
                 | resource. In some other cases with high 1_Port_Utilized and
                 | L1 Bound, this metric can point to L1 data-cache latency
                 | bottleneck that may not necessarily manifest with complete
                 | execution starvation (due to the short L1 latency e.g.
                 | walking a linked list) - looking at the assembly can be
                 | helpful. Note that this metric value may be highlighted due
                 | to L1 Bound issue.
                 |
                Cycles of 2 Ports Utilized: 20.0% of Clockticks
                 | This metric represents cycles fraction CPU executed total of
                 | 2 uops per cycle on all execution ports (Logical Processor
                 | cycles since ICL, Physical Core cycles otherwise). Tip: Loop
                 | Vectorization - most compilers feature auto-Vectorization
                 | options today- reduces pressure on the execution ports as
                 | multiple elements are calculated with same uop.
                 |
                Cycles of 3+ Ports Utilized: 38.5% of Clockticks
                    ALU Operation Utilization: 60.0% of Clockticks
                        Port 0: 48.9% of Clockticks
                        Port 1: 53.6% of Clockticks
                        Port 5: 50.7% of Clockticks
                        Port 6: 86.7% of Clockticks
                    Load Operation Utilization: 51.6% of Clockticks
                    Store Operation Utilization: 7.7% of Clockticks
    Average CPU Frequency: 3.216 GHz
    Total Thread Count: 3,138
    Paused Time: 0s
Effective CPU Utilization: 14.4%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 9.223 out of 64
Collection and Platform Info
    Application Command Line: numactl "--cpunodebind=0" "--membind=0" "./winograd" "conf/vgg16.conf" 
    Operating System: 5.15.0-100-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
    Computer Name: hepnode1
    Result Size: 123.9 MB 
    Collection start time: 08:39:33 25/03/2025 UTC
    Collection stop time: 08:40:06 25/03/2025 UTC
    Collector Type: Driverless Perf system-wide sampling
    CPU
        Name: Intel(R) Xeon(R) Processor code named Icelake
        Frequency: 1.995 GHz
        Logical CPU Count: 64
        LLC size: 50.3 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
数据收集成功！生成报告...
分析完成: uarch-exploration
结果保存在: vtune_results/v0.3.2.71-vgg16/uarch-exploration_vgg16_20250325_163604



===============================================
 所有分析完成！
 结束时间: Tue Mar 25 04:40:20 PM CST 2025
 结果位置: vtune_results/v0.3.2.71-vgg16
===============================================
创建了索引文件: vtune_results/v0.3.2.71-vgg16/index.html
正在启动 VTune 后端服务，端口: 8080...
VTune 后端服务已启动，PID: 1534988
可通过浏览器访问: http://hepnode1:8080
后端服务将持续运行12小时，之后自动停止
VTune 分析完成。可使用以下命令停止后端服务:
kill $(cat vtune_results/v0.3.2.71-vgg16/backend_pid.txt)
